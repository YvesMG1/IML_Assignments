{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore convergence warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    This function loads the data from the csv files and returns it as numpy arrays.\n",
    "\n",
    "    input: None\n",
    "    \n",
    "    output: x_pretrain: np.ndarray, the features of the pretraining set\n",
    "            y_pretrain: np.ndarray, the labels of the pretraining set\n",
    "            x_train: np.ndarray, the features of the training set\n",
    "            y_train: np.ndarray, the labels of the training set\n",
    "            x_test: np.ndarray, the features of the test set\n",
    "    \"\"\"\n",
    "    x_pretrain = pd.read_csv(\"pretrain_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_pretrain = pd.read_csv(\"pretrain_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_train = pd.read_csv(\"train_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1).to_numpy()\n",
    "    y_train = pd.read_csv(\"train_labels.csv.zip\", index_col=\"Id\", compression='zip').to_numpy().squeeze(-1)\n",
    "    x_test = pd.read_csv(\"test_features.csv.zip\", index_col=\"Id\", compression='zip').drop(\"smiles\", axis=1)\n",
    "    return x_pretrain, y_pretrain, x_train, y_train, x_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 layer autoencoder\n",
    "class AutoEncoder1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder1, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1000)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        if self.training:\n",
    "            x = self.decoder(x)\n",
    "        else:\n",
    "            x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_extractor(x, y, model = AutoEncoder1(), batch_size=256, eval_size=1000, lr=0.01, weight_decay = 0.0001, patience=5, alpha=0.4, step_size=10, gamma=0.5):\n",
    "    \"\"\"\n",
    "    This function trains the feature extractor on the pretraining data and returns a function which\n",
    "    can be used to extract features from the training and test data.\n",
    "\n",
    "    input: x: np.ndarray, the features of the pretraining set\n",
    "              y: np.ndarray, the labels of the pretraining set\n",
    "                batch_size: int, the batch size used for training\n",
    "                eval_size: int, the size of the validation set\n",
    "            \n",
    "    output: make_features: function, a function which can be used to extract features from the training and test data\n",
    "    \"\"\"\n",
    "    # Pretraining data loading\n",
    "    in_features = x.shape[-1]\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x, y, test_size=eval_size, random_state=10, shuffle=True)\n",
    "    x_tr, x_val = torch.tensor(x_tr, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "    y_tr, y_val = torch.tensor(y_tr, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "\n",
    "    #reshaping\n",
    "    y_tr = y_tr.view(-1, 1)\n",
    "    y_val = y_val.view(-1, 1)\n",
    "    \n",
    "    # Data loading\n",
    "    train_data = TensorDataset(x_tr, y_tr)\n",
    "    val_data = TensorDataset(x_val, y_val)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # model declaration\n",
    "    model = model\n",
    "    \n",
    "    # Training parameters\n",
    "    classification_loss = nn.MSELoss()\n",
    "    reconstruction_loss = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    epochs = 50\n",
    "    patience = patience\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    alpha = 0.4\n",
    "\n",
    "    # Initialize arrays for plotting\n",
    "    train_recon_time = np.array([])\n",
    "    train_class_time = np.array([])\n",
    "    train_total_time = np.array([])\n",
    "\n",
    "    vali_recon_time = np.array([])\n",
    "    vali_class_time = np.array([])\n",
    "    vali_total_time = np.array([])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # training\n",
    "        total_train_reconstruction = 0\n",
    "        total_train_classification = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Reconstruction loss\n",
    "            model.train()\n",
    "            outputs = model(x_batch)\n",
    "            recon_loss = reconstruction_loss(outputs, x_batch)\n",
    "            total_train_reconstruction += recon_loss.item() * x_batch.size(0)\n",
    "\n",
    "            # Classification loss\n",
    "            model.eval()\n",
    "            outputs = model(x_batch)\n",
    "            class_loss = classification_loss(outputs, y_batch)\n",
    "            total_train_classification += class_loss.item() * x_batch.size(0)\n",
    "\n",
    "            # Total loss\n",
    "            loss = alpha * recon_loss + (1 - alpha) * class_loss\n",
    "            total_train += loss.item() * x_batch.size(0)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_train_reconstruction /= len(train_loader.dataset)\n",
    "        total_train_classification /= len(train_loader.dataset)\n",
    "        total_train /= len(train_loader.dataset)\n",
    "\n",
    "        train_recon_time = np.append(train_recon_time, total_train_reconstruction)\n",
    "        train_class_time = np.append(train_class_time, total_train_classification)\n",
    "        train_total_time = np.append(train_total_time, total_train)\n",
    "\n",
    "        # validation\n",
    "        total_vali_reconstruction = 0\n",
    "        total_vali_classification = 0\n",
    "        total_vali = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "\n",
    "                # Reconstruction loss\n",
    "                model.train()\n",
    "                outputs = model(x_batch)\n",
    "                recon_loss = reconstruction_loss(outputs, x_batch)\n",
    "                total_vali_reconstruction += recon_loss.item() * x_batch.size(0)\n",
    "\n",
    "                # Classification loss\n",
    "                model.eval()\n",
    "                outputs = model(x_batch)\n",
    "                class_loss = classification_loss(outputs, y_batch)\n",
    "                total_vali_classification += class_loss.item() * x_batch.size(0)\n",
    "\n",
    "                # Total loss\n",
    "                loss = alpha * recon_loss + (1 - alpha) * class_loss\n",
    "                total_vali += loss.item() * x_batch.size(0)\n",
    "\n",
    "        total_vali_reconstruction /= len(val_loader.dataset)\n",
    "        total_vali_classification /= len(val_loader.dataset)\n",
    "        total_vali /= len(val_loader.dataset)\n",
    "\n",
    "        vali_recon_time = np.append(vali_recon_time, total_vali_reconstruction)\n",
    "        vali_class_time = np.append(vali_class_time, total_vali_classification)\n",
    "        vali_total_time = np.append(vali_total_time, total_vali)\n",
    "\n",
    "        # early stopping\n",
    "        if total_vali < best_val_loss:\n",
    "            best_val_loss = total_vali\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        # learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    def make_features(x, num_layers = 2):\n",
    "        \"\"\"\n",
    "        This function extracts features from the training and test data, used in the actual pipeline \n",
    "        after the pretraining.\n",
    "\n",
    "        input: x: np.ndarray, the features of the training or test set\n",
    "\n",
    "        output: features: np.ndarray, the features extracted from the training or test set, propagated\n",
    "        further in the pipeline\n",
    "        \"\"\"\n",
    "        # TODO: Implement the feature extraction, a part of a pretrained model used later in the pipeline.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            new_model = nn.Sequential(*list(model.encoder.children())[:-num_layers])\n",
    "            features = new_model(torch.tensor(x, dtype=torch.float))\n",
    "        return features.detach().numpy()\n",
    "\n",
    "    return make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(x, y):\n",
    "        kfold = KFold(n_splits=10, shuffle=True, random_state=10)\n",
    "        \n",
    "        models= [Ridge(fit_intercept=True, random_state=10),\n",
    "                Lasso(fit_intercept=True, random_state=10),\n",
    "                ElasticNet(fit_intercept=True, random_state=10)]\n",
    "\n",
    "        # Define set of possible hyperparameter values\n",
    "        grid = {\"Ridge\": {\"alpha\": [0.01, 0.1, 0.5, 1, 5, 10, 15, 20]},\n",
    "                \"Lasso\": {\"alpha\": [0.001, 0.01, 0.1, 1, 10]},\n",
    "                \"ElasticNet\": {\"alpha\": [0.001, 0.01, 0.1, 1, 10],\n",
    "                        \"l1_ratio\": [0.2, 0.4, 0.6, 0.8]}}\n",
    "\n",
    "        algorithm = [\"Ridge\", \"Lasso\", \"ElasticNet\"]\n",
    "\n",
    "        # Apply Grid search and add for each model the best score and the respective parameter to the list\n",
    "        gs_bestscore = []\n",
    "        gs_bestpara = []\n",
    "        for i, model in enumerate(models):\n",
    "                gs = GridSearchCV(model, param_grid = grid[algorithm[i]], cv=kfold, scoring=\"neg_root_mean_squared_error\")\n",
    "                gs.fit(x, y)\n",
    "                gs_bestscore.append(gs.best_score_)\n",
    "                gs_bestpara.append(gs.best_estimator_)\n",
    "\n",
    "        return max(gs_bestscore), gs_bestpara[gs_bestscore.index(max(gs_bestscore))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder1() # Change to assigned autoencoder\n",
    "x_pretrain, y_pretrain, x_train, y_train, x_test = load_data()\n",
    "\n",
    "feature_extractor =  make_feature_extractor(x_pretrain, y_pretrain)\n",
    "x_train_t = feature_extractor(x_train)\n",
    "best_score, best_para = linear_regression(x_train_t, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
